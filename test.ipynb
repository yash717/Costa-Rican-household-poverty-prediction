{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"train.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cleaning columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns[df.isnull().sum()>0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.fillna(method='bfill',inplace=True)\n",
    "df.columns[df.isnull().sum()>0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.fillna(method=\"ffill\",inplace=True)\n",
    "df.columns[df.isnull().sum()>0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding numberic and categoical columns\n",
    "numeric_columns=df.select_dtypes(include=np.number).columns.tolist()\n",
    "categorical_columns=set(df.columns).difference(set(numeric_columns))\n",
    "print(categorical_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['edjefa']=df['edjefa'].replace({\"no\":0,\"yes\":1}).astype(float)\n",
    "df['edjefe']=df['edjefe'].replace({\"no\":0,\"yes\":1}).astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#finding numeric and categorical cols\n",
    "numeric_columns=df.select_dtypes(include=np.number).columns.tolist()\n",
    "categorical_columns=set(df.columns).difference(set(numeric_columns))\n",
    "print(categorical_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['edjefe'].unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['edjefa'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['edjefa'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['dependency']=np.sqrt(df['SQBdependency'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum().sum()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_drops=['Id','idhogar']\n",
    "df.drop(col_drops,axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_columns=df.select_dtypes(include=np.number).columns.tolist()\n",
    "categorical_columns=set(df.columns).difference(set(numeric_columns))\n",
    "print(categorical_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## increasing features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set=df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['rent_per_adult']=df['v2a1']/df['hogar_adul']\n",
    "df['rent_per_person']=df['v2a1']/df['hhsize']\n",
    "df['overcrowding_room_and_bedroom']=(df['hacdor']/df['hacapo'])/2\n",
    "df['no_appliances']=df['refrig']+df['computer']+df['television']\n",
    "df['r4h1_percent_in_male']=df['r4h1']/df['r4h3']\n",
    "df['r4m1_percent_in_female']=df['r4m1']/df['r4m3']\n",
    "df['r4h1_percent_in_total']=df['r4h1']/df['hhsize']\n",
    "df['r4m1_percent_in_total']=df['r4m1']/df['hhsize']\n",
    "df['r4t1_percent_in_total']=df['r4t1']/df['hhsize']\n",
    "\n",
    "df['adult']=df['hogar_adul']-df['hogar_mayor']\n",
    "df['dependency_count']=train_set['hogar_nin']+df['hogar_mayor']\n",
    "df['dependency']=df['dependency_count']/df['adult']\n",
    "df['child_percent']=df['hogar_nin']/df['hogar_total']\n",
    "df['elder_percent']=df['hogar_mayor']/df['hogar_total']\n",
    "df['adult_percent']=df['hogar_adul']/df['hogar_total']\n",
    "\n",
    "df['rent_per_bedroom']=df['v2a1']/df['bedrooms']\n",
    "df['adults_per_bedroom']=df['adult']/df['bedrooms']\n",
    "df['child_per_bedroom']=df['hogar_nin']/df['bedrooms']\n",
    "df['male_per_bedroom']=df['r4h3']/df['bedrooms']\n",
    "df['female_per_bedroom']=df['r4m3']/df['bedrooms']\n",
    "\n",
    "df['bedrooms_per_person_household']=df['hhsize']/df['bedrooms']\n",
    "df['tablet_per_person_household']=df['v18q1']/df['hhsize']\n",
    "df['bedrooms_per_person_household']=df['qmobilephone']/df['hhsize']\n",
    "\n",
    "df['age_12_19']=df['hogar_nin']-df[\"r4t1\"]\n",
    "\n",
    "df['rent_per_room']=df['v2a1']/df['rooms']\n",
    "df['bedroom_per_room']=df['bedrooms']/df['rooms']\n",
    "df['elder_per_room']=df['hogar_mayor']/df['rooms']\n",
    "df['adults_per_room']=df['adult']/df['rooms']\n",
    "df['child_per_room']=df['hogar_nin']/df['rooms']\n",
    "df['male_per_room']=df['r4h3']/df['rooms']\n",
    "df['female_per_room']=df['r4m3']/df['rooms']\n",
    "df['room_per_person_household']=df['hhsize']/df['rooms']\n",
    "df['escolari_age']=df['escolari']/df['age']\n",
    "\n",
    "df['rez_esc_escolari']=df['rez_esc']/df['escolari']\n",
    "df['rez_esc_r4t1']=df['rez_esc']/df['r4t1']\n",
    "df['rez_esc_r4t2']=df['rez_esc']/df['r4t2']\n",
    "df['rez_esc_r4t3']=df['rez_esc']/df['r4t3']\n",
    "df['rez_esc_age']=df['rez_esc']/df['age']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import LGBMRegressor\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from bayes_opt import BayesianOptimization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_1=df.drop('Target',axis=1)rt XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_1=df.drop('Target',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_1.shape\n",
    "x_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=df['Target']\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "# Load your data and define x_1 and y\n",
    "\n",
    "# Check for NaN or infinite values in y\n",
    "nan_indices_y = np.where(np.isnan(y))\n",
    "inf_indices_y = np.where(np.isinf(y))\n",
    "print(\"NaN indices in y:\", nan_indices_y)\n",
    "print(\"Infinite indices in y:\", inf_indices_y)\n",
    "\n",
    "# Check for NaN or infinite values in x_1\n",
    "nan_indices_x_1 = np.where(np.isnan(x_1))\n",
    "inf_indices_x_1 = np.where(np.isinf(x_1))\n",
    "print(\"NaN indices in x_1:\", nan_indices_x_1)\n",
    "print(\"Infinite indices in x_1:\", inf_indices_x_1)\n",
    "\n",
    "# Remove rows with NaN or infinite values in x_1 and y\n",
    "clean_indices = np.logical_and.reduce((~np.isnan(x_1).any(axis=1), ~np.isinf(x_1).any(axis=1), ~np.isnan(y), ~np.isinf(y)))\n",
    "x_1_clean = x_1[clean_indices]\n",
    "y_clean = y[clean_indices]\n",
    "\n",
    "# Normalize or scale x_1\n",
    "scaler = StandardScaler()\n",
    "x_1_scaled = scaler.fit_transform(x_1_clean)\n",
    "\n",
    "def xgb_cv(n_estimators, max_depth, gamma, subsample, data, targets):\n",
    "    estimator = XGBClassifier(n_estimators=int(n_estimators), max_depth=int(max_depth), gamma=gamma, subsample=subsample, random_state=2)\n",
    "    cval = cross_val_score(estimator, data, targets, cv=5)\n",
    "    return cval.mean()\n",
    "\n",
    "def optimize_xgb(data, targets):\n",
    "    def xgb_crossval(n_estimators, max_depth, gamma, subsample):\n",
    "        return xgb_cv(\n",
    "            n_estimators=n_estimators,\n",
    "            max_depth=max_depth,\n",
    "            gamma=gamma,\n",
    "            subsample=subsample,\n",
    "            data=data, targets=targets)\n",
    "    optimizer = BayesianOptimization(f=xgb_crossval, pbounds={\n",
    "        \"n_estimators\": (100, 500),\n",
    "        \"max_depth\": (6, 15),\n",
    "        \"gamma\": (0, 10),\n",
    "        \"subsample\": (0.0, 1.0)\n",
    "    },\n",
    "    random_state=1234, verbose=2)\n",
    "    optimizer.maximize(n_iter=25, init_points=10)\n",
    "    print(\"final results: \", optimizer.max)\n",
    "\n",
    "# Call the optimize_xgb function\n",
    "print(\"..optimizing xgboost...\")\n",
    "optimize_xgb(x_1_scaled, y_clean)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "# Load your data and define x_1 and y\n",
    "\n",
    "# Check for NaN or infinite values in y\n",
    "nan_indices_y = np.where(np.isnan(y))\n",
    "inf_indices_y = np.where(np.isinf(y))\n",
    "print(\"NaN indices in y:\", nan_indices_y)\n",
    "print(\"Infinite indices in y:\", inf_indices_y)\n",
    "\n",
    "# Check for NaN or infinite values in x_1\n",
    "nan_indices_x_1 = np.where(np.isnan(x_1))\n",
    "inf_indices_x_1 = np.where(np.isinf(x_1))\n",
    "print(\"NaN indices in x_1:\", nan_indices_x_1)\n",
    "print(\"Infinite indices in x_1:\", inf_indices_x_1)\n",
    "\n",
    "# Remove rows with NaN or infinite values in x_1 and y\n",
    "clean_indices = np.logical_and.reduce((~np.isnan(x_1).any(axis=1), ~np.isinf(x_1).any(axis=1), ~np.isnan(y), ~np.isinf(y)))\n",
    "x_1_clean = x_1[clean_indices]\n",
    "y_clean = y[clean_indices]\n",
    "\n",
    "# Normalize or scale x_1\n",
    "scaler = StandardScaler()\n",
    "x_1_scaled = scaler.fit_transform(x_1_clean)\n",
    "\n",
    "# Define the xgb_cv and optimize_xgb functions\n",
    "def xgb_cv(n_estimators, max_depth, gamma, subsample, data, targets):\n",
    "    estimator = XGBClassifier(n_estimators=int(n_estimators), max_depth=int(max_depth), gamma=gamma, subsample=subsample, random_state=2)\n",
    "    cval = cross_val_score(estimator, data, targets, cv=5)\n",
    "    return cval.mean()\n",
    "\n",
    "def optimize_xgb(data, targets):\n",
    "    def xgb_crossval(n_estimators, max_depth, gamma, subsample):\n",
    "        return xgb_cv(\n",
    "            n_estimators=n_estimators,\n",
    "            max_depth=max_depth,\n",
    "            gamma=gamma,\n",
    "            subsample=subsample,\n",
    "            data=data, targets=targets)\n",
    "    optimizer = BayesianOptimization(f=xgb_crossval, pbounds={\n",
    "        \"n_estimators\": (100, 500),\n",
    "        \"max_depth\": (6, 15),\n",
    "        \"gamma\": (0, 10),\n",
    "        \"subsample\": (0.0, 1.0)\n",
    "    },\n",
    "    random_state=1234, verbose=2)\n",
    "    optimizer.maximize(n_iter=25, init_points=10)\n",
    "    print(\"final results: \", optimizer.max)\n",
    "\n",
    "# Call the optimize_xgb function\n",
    "print(\"..optimizing xgboost...\")\n",
    "optimize_xgb(x_1_scaled, y_clean)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from catboost import CatBoostClassifier\n",
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "# Define the cb_cv and optimize_cb functions\n",
    "def cb_cv(n_estimators, depth, learning_rate, data, targets):\n",
    "    estimator = CatBoostClassifier(\n",
    "        n_estimators=int(n_estimators),\n",
    "        depth=int(depth),\n",
    "        learning_rate=learning_rate,\n",
    "        random_state=2,\n",
    "        verbose=0\n",
    "    )\n",
    "    cval = cross_val_score(estimator, data, targets, cv=5)\n",
    "    return cval.mean()\n",
    "\n",
    "def optimize_cb(data, targets):\n",
    "    def cb_crossval(n_estimators, depth, learning_rate):\n",
    "        return cb_cv(\n",
    "            n_estimators=n_estimators,\n",
    "            depth=depth,\n",
    "            learning_rate=learning_rate,\n",
    "            data=data, targets=targets\n",
    "        )\n",
    "    optimizer = BayesianOptimization(f=cb_crossval, pbounds={\n",
    "        \"n_estimators\": (100, 500),\n",
    "        \"depth\": (4, 16),\n",
    "        \"learning_rate\": (0.001, 0.1)\n",
    "    },\n",
    "    verbose=2)\n",
    "    optimizer.maximize(n_iter=5, init_points=12,)\n",
    "    print(\"final results: \", optimizer.max)\n",
    "\n",
    "# Assuming you have 'x_1_scaled' and 'y_clean' defined earlier\n",
    "print(\"..optimizing cb...\")\n",
    "optimize_cb(x_1_scaled, y_clean)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from lightgbm import LGBMClassifier\n",
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "# Define the lgb_cv and optimize_lgb functions\n",
    "def lgb_cv(n_estimators, num_leaves, min_child_samples, subsample, data, targets):\n",
    "    estimator = LGBMClassifier(\n",
    "        n_estimators=int(n_estimators),\n",
    "        num_leaves=int(num_leaves),\n",
    "        min_child_samples=int(min_child_samples),\n",
    "        subsample=subsample,\n",
    "        boosting_type='gbdt',  # Change to 'dart' or 'goss' if desired\n",
    "        learning_rate=0.1,     # Adjust learning rate\n",
    "        random_state=2,\n",
    "        n_jobs=-1              # Use all available CPU cores\n",
    "    )\n",
    "    cval = cross_val_score(estimator, data, targets, cv=5)\n",
    "    return cval.mean()\n",
    "\n",
    "def optimize_lgb(data, targets):\n",
    "    def lgb_crossval(n_estimators, num_leaves, min_child_samples, subsample):\n",
    "        return lgb_cv(\n",
    "            n_estimators=n_estimators,\n",
    "            num_leaves=num_leaves,\n",
    "            min_child_samples=min_child_samples,\n",
    "            subsample=subsample,\n",
    "            data=data,\n",
    "            targets=targets\n",
    "        )\n",
    "    optimizer = BayesianOptimization(f=lgb_crossval, pbounds={\n",
    "        \"n_estimators\": (100, 3000),   # Reduce the range for faster optimization\n",
    "        \"num_leaves\": (20, 200000),        # Reduce the range for faster optimization\n",
    "        \"min_child_samples\": (10, 30), # Reduce the range for faster optimization\n",
    "        \"subsample\": (0.5, 1.0)\n",
    "    },\n",
    "    random_state=1234,\n",
    "    verbose=2)\n",
    "    optimizer.maximize(n_iter=100, init_points=3)  # Reduce n_iter and init_points\n",
    "    print(\"final results: \", optimizer.max)\n",
    "\n",
    "# Assuming you have 'x_1_scaled' and 'y_clean' defined earlier\n",
    "print(\"..optimizing lgb...\")\n",
    "optimize_lgb(x_1_scaled, y_clean)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "xg=XGBClassifier()\n",
    "Lgbm=LGBMClassifier(n_estimators=100,learning_rate=0.1,random_state=42,num_leaves=200)\n",
    "cataboost=CatBoostClassifier(depth=int(9.242),n_estimators=int(514.1))\n",
    "estimators=[('catboost',catboost),('Xg',Xg)]\n",
    "clf=StackingClassifier(estimators=estimators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train,x_test,y_train,y_test=train_test_split(x_1,y,random_state=42)\n",
    "clf.fit(x_train,y_train)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.score(x_test,y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
